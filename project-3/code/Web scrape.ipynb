{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f1caf0",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f51d0",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "Singapore has recently lifted all its Covid-19 pre-departure testing in Feb 2023 as part of the big move to lift its remaining border measures at the end of the pandemic. As a Data Analyst hired by a local travel agency, we are tasked to research on some of the latest popular travel destinations in order to assist the marketing and operations team in their campaign to promote tour package services in these countries for the upcoming 2023 Travel Fair. In order to ensure a successful campaign, we would need to gather essential reviews on the latest hit attractions that are currently trending among Singaporeans. Our preliminary research studies have shown that Japan and Thailand have emerged as one of the top few holiday destinations for Singaporeans. In this project, we will be attempting to build a binary NLP classifer model that can correctly categorise these reviews into both countries.\n",
    "\n",
    "As a novel apporach, text information from subreddits r/JapanTravel and r/ThailandTourism will be scraped using Reddit's Pushshift API to collect the necesary data and train the model. Further analysis on the text data, as well as evaluation on the model's ability to successfully classify the corresponding texts would be elaborated throughout the notebook\n",
    "\n",
    "Part 1 consists of Web scraping the text information, compiling and saving the raw dataset to be used in Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a294cb2",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "1. https://www.straitstimes.com/life/travel/bangkok-tokyo-and-bali-are-top-year-end-destinations-among-singapore-travellers\n",
    "2. https://www.travelandleisureasia.com/sg/news/year-end-travel-destinations-among-singapore-travellers/\n",
    "3. https://www.straitstimes.com/singapore/singapore-will-lift-remaining-covid-19-border-restrictions-from-feb-13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f041db0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a220c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d96f4",
   "metadata": {},
   "source": [
    "**Scraped on Wednesday, 13 March 2023 15:25:14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8223a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape both subreddits using Pushsift API\n",
    "def scrape(cat1, num, cat2):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/46.0.2490.80'}\n",
    "    #Pushshift API URL\n",
    "    url1 = f'https://api.pushshift.io/reddit/search/submission?subreddit={cat1}'\n",
    "    #Initiate empty timestamp\n",
    "    before_timestamp = None\n",
    "    #Pushshift API params for 1st subreddit forum, defined to scrape 150 post per scrape\n",
    "    params1 = {'subreddit': cat1, 'size': 150, before_timestamp: before_timestamp}\n",
    "    #create empty dataframe\n",
    "    df1 = pd.DataFrame([],columns=['subreddit', 'created_utc', 'title', 'selftext'])\n",
    "    #while loop to request information from API and scrape the posts\n",
    "    while len(df1.index) < num:\n",
    "        res1 = requests.get(url1, params1, headers=headers)\n",
    "        posts1 = res1.json()[\"data\"]\n",
    "        posts1_df = pd.DataFrame(posts1, columns=['subreddit', 'created_utc', 'title', 'selftext'])\n",
    "        df1 = pd.concat([df1, posts1_df])\n",
    "        before_timestamp = posts1[-1]['created_utc']\n",
    "        params1['before'] = before_timestamp\n",
    "    #Pushshift API params for 2nd subreddit forum, defined to scrape 150 post per scrape\n",
    "    url2 = f'https://api.pushshift.io/reddit/search/submission?subreddit={cat2}'\n",
    "    params2 = {'subreddit': cat2, 'size': 150, 'before': before_timestamp}\n",
    "    df2 = pd.DataFrame([],columns=['subreddit', 'created_utc', 'title', 'selftext'])\n",
    "    while len(df2.index) < num:\n",
    "        res2 = requests.get(url2, params2, headers=headers)\n",
    "        posts2 = res2.json()[\"data\"]\n",
    "        posts2_df = pd.DataFrame(posts2, columns=['subreddit', 'created_utc', 'title', 'selftext'])\n",
    "        df2 = pd.concat([df2, posts2_df])\n",
    "        before_timestamp = posts2[-1]['created_utc']\n",
    "        params2['before'] = before_timestamp\n",
    "    \n",
    "    final_df = pd.concat([df1, df2])\n",
    "    final_df.drop(columns=['created_utc'], inplace=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d23ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_df = scrape('japantravel', 1000, 'ThailandTourism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053e1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_df.to_csv('../data/japan_vs_thailand.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
